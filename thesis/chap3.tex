\chapter{Methods} This chapter describes the two main strategies I implemented
for incrementally training random forests. I go into depth about the details of
each implementation, hybrid strategies, and the API I exposed for use in Spark
ML pipelines. My experiments seek to shed insight into which algorithms work
best on different workloads.

\section{Tree regeneration strategy}

The first implemented strategy involves regrowing trees within the random
forest. For every new incremental batch, the exposed Spark API allows an
existing model to be updated with the new data. A set proportion of trees are
randomly selected for replacement. The system then grows a random forest with
the number of trees selected, creating a new hybrid model with the
maintained trees from the old model and the newly grown trees from the new
model. To allow for the creation of hybrid models, I introduce an incremental
random forest classification model. This model tracks metadata from
previously-seen data and information about each tree in the forest. The
incremental random forest classifier heavily relies on this metadata to
generate summary data structures for new data batches. The stored metadata
enforces a degree of consistency across the information stored in the leaves of
the forest, allowing my implementation to use the existing infrastructure in
Spark's original random forest classifier. As a result, my implementation can
intermix tree regeneration with incremental growth, as described in the next
section below.

\section{Incremental growth strategy}

The second implemented strategy involves incrementally growing existing trees
in the forest, and is based off of the Saffari and Denil algorithms. A user can
update an existing incremental random forest classification model with a new
batch of data via the exposed Spark API. Because this model tracks metadata
from previous batches of data, the system can determine the candidate splits
that were considered during previous growth phases of the tree. The new data is
divided into bins specified by the previously-calculated splits and used to
split leaves of the tree. As a result, the system avoids passing around a new
batch of data in its entirety; instead, it uses generated bin count metadata to
increase efficiency.

I augmented the RandomForest class in Spark ML to store metadata about past
runs in leaves. After the new batch of data is processed into bins, I merge
this information with the information stored in each leaf. Because the bin
metadata merely stores aggregate counts of the points in each bin, metadata can
be merged by adding entries in one metadata structure to the corresponding
slots in the other metadata. To provide leaves with ability to store metadata
and merge in new batches of data, I implemented a custom incremental leaf node
class that replaces the customary Spark-provided leaf node class. This class
maintains the capabilities of the original Spark leaf node, but allows for
changes to be made to each tree in the random forest.

Once the new metadata is aggregated at each leaf, the system splits leaves that
newly pass both requisite thresholds: the minimum number of points a leaf must
account for and the minimum information gain from the split. After each tree is
incrementally grown, the system returns a new incremental random forest model
with updated metadata to the user.

\section{Optimizations}

In this section, I detail the optimizations I made to my implementation of the
incremental growth and tree replacement strategies. These optimizations are
additions to the memory and speed optimizations already existing in ordinary
Spark random forest classifiers.

\subsection{Bounding incremental growth}

Since datasets can have many features, unbounded tree growth can result in
extremely deep trees after just a few batches. The largest tree height
supported by Spark ML is 30, after which trees can no longer be grown
incrementally. Additionally, splitting leaves in tall trees imposes a large
performance overhead, as the algorithm must examine a large number of candidate
leaves. As a result, the system restricts the maximum initial tree height to a
set value $h$, relaxing this restriction by one for each incrementally grown
tree that receives a new batch of data. In all hybrid strategies, a tree is
replaced when it reaches a certain threshold depth. When a tree is regrown, its
maximum height is reset to the original value $h$. This technique prevents the
incremental random forest from overfitting to earlier batches and allows the
forest to better adapt to concept drift, as it ensures that later batches can
significantly impact tree predictions.

\subsection{Batched processing}

The incremental random forest implementations in the Saffari and Denil papers
describe accumulating points in leaves until the leaf is split. In the purely
online setting, accumulating points one at a time is the only option. Since
batched workloads allow for many points to be processed at one time, we can
preprocess the batch into metadata structures that are passed to the relevant
leaves and merged with the existing leaf metadata. This optimization prevents
the system from having to run back over all previous batches to decide on a
split.

\subsection{Tree reweighting}

In the batched incremental learning setting, the classifier receives an
unlabeled batch of points, predicts the label for each point, and then views
the labels to assess its accuracy. Each tree in the random forest has a
different accuracy on a particular new batch of data. To increase the overall
accuracy of the forest, after the system assesses the accuracy on each new
batch, it weights the trees with lower accuracy to have less voting power on
the next batch. Because this increases the chances of overfitting, the system
restricts the extent to which tree weights can shift.

To account for concept drift, I weight trees by age. With every new batch,
trees that are not regrown are weighted to have less voting power. As a result,
the classifier captures information from previous batches, but uses information
from the most recent batch more heavily in the classification of the next
batch.

\subsection{Leaf subsampling}

As tree height increases, the amount of computation required to grow the tree
one level increases significantly. However, adapting an existing tree to a new
batch of data does not necessarily require splitting every leaf in the the tree
on the new data. Instead, the system can select a random sample of the leaves
in the tree, and only consider these leaves for incremental growth. While this
optimization decreases the performance of shallow random forests, for deep
forests, it helps improve the training time without significantly impacting the
performance.

% \subsection{Extremely Randomized Trees}

% Some papers in the literature incorporate extremely randomized trees, rather than decision trees that choose optimal splits, into their random forest classifiers. \cite{Saffari} \cite{NIPS2014_5234} 

\section{Experiments}

The tree regeneration and incremental growth strategies are compatible; both
can be used simultaneously within an incremental random forest. This thesis
explores various hybrid approaches and provides insight into which approaches
work best on datasets with and without concept drift. For each hybrid approach,
I vary the proportion of trees that are grown with each strategy. The system
selects trees without replacement to be replaced or grown incrementally; the
remaining trees are unchanged. 

In my research, I experiment with several different parameters that affect the
error rate of my incremental random forest classifier. I hypothesize that
certain parameters used to build each incremental random forest will affect the
optimal incremental strategy for that forest on my test datasets. I vary the
following parameters in my experiments and report the effect on the optimal
hybrid incremental algorithm.

\begin{enumerate}

    \item \textbf{Tree depth.} I vary the initial maximum height of each tree
      in the random forest. In my experiments, I use two initial heights:
      shallow and deep, which represent heights of 5 and 10, respectively. I
      chose 10 to be the sample height for deep trees, as it is the most
      commonly used height in the literature to obtain a high accuracy without
      overfitting. A shallow tree height of 5 still provides some accuracy, but
      allows the tree to adapt more significantly to concept drift in
      incremental batches.

    \item \textbf{Concept drift.} I test the performance of the incremental
      random forest classifier on four datasets: two with concept drift and two
      without concept drift. The datasets with concept drift show seasonal
      changes over the course of many months, whereas the datasets without
      concept drift are randomized as to ensure that there is no significant
      shift in data distribution.

    \item \textbf{Batch size.} I vary the batch size of the data workloads to
      test the impact of batch size on classifier performance and optimal
      strategy. Specifically, I select 100 as the ``small'' batch size and
      2,000 as the ``large'' batch size. 

    \item \textbf{Hybrid strategy.} I vary the proportion of trees grown
      incrementally between 0.0 and 1.0, using intermediate steps of 0.1. I
      also vary the proportion of trees regrown after each batch from 0.0 to
      1.0 with step size 0.1. No results are recorded where the sum of these two
      proporsions is invalid---i.e. exceed 1.0. 
\end{enumerate}

Through my experiments, I seek to diagnose the optimal hybrid approach for each
of four data workloads, shown in Table \ref{tab:workloads}. To evaluate the
efficiency and accuracy of a classifier, I measure the training time and
predictive error rate on the next batch. I compare the metrics taken on random
forest classifiers running a variety of incremental strategies to the control
setting, which involves regrowing the entire random forest from scratch on all
batches previously seen. The results chapter of this thesis discusses the
results for each workload separately, and then explains an overarching method
for selecting a general hybrid strategy for a new workload.

\begin{table}
  \centering
  \begin{tabular}{ | c | c | c | }
    \hline
    \textbf{Workload} & \textbf{Batch size} & \textbf{Concept Drift} \\
    \hline
    Homesite Quote Conversion & large & no \\
    \hline
    Otto Group Product Classification & small & no \\
    \hline
    Airline Delay Causes & large & yes \\
    \hline
    Bike Sharing & small & yes \\
    \hline
  \end{tabular}
  \caption{I run experiments on the four example workloads listed in this table. These workloads span the combinations of small and large batch size with existent or nonexistent concept drift.}
  \label{tab:workloads}
\end{table}

\section{API}

My custom incremental random forest classifier class,
IncrementalRandomForestClassifier, exposes a Scala API for data scientists to
use. I introduce a new model class, the
IncrementalRandomForestClassificationModel class, which is an augmented
adaptation of the Spark ML RandomForestClassificationModel class that allows
for incremental optimizations. Table \ref{tab:api} details the methods
available for use in Spark ML pipelines. The code for my implementation of
incremental random forest classifiers has been open-sourced on Github.

\begin{table}
  \centering
  \begin{tabular}{ | c | c | }
  \hline
  \textbf{Method} & \textbf{Description} \\
  \hline
  def train(df: DataFrame): & Trains a model for future \\
  IncrementalRFClassificationModel &  online learning by maintaining \\
  & tree and leaf metadata. \\ 
  \hline
  def update( & Updates an existing model \\
  old: IncrementalRFClassificationModel, & with a new batch of data \\
  df: DataFrame): & by regrowing some trees and \\
  IncrementalRFClassificationModel & incrementally growing others. \\
  \hline
  def addTrees( & Trains a model using a warm \\
  oldModel: IncrementalRFClassificationModel, & start by adding more \\
  df: DataFrame, addedTrees: Int): & trees to the existing forest. \\
  IncrementalRFClassificationModel & \\ 
  \hline
  def setRegrowProportion(prop: Int) & Set the proportion of trees \\
  & in the forest that should be \\
  & regrown with every new batch. \\
  \hline
  def setIncrementalProportion(prop: Int) & Set the proportion of trees \\
  & in the forest that should be \\
  & regrown with every new batch. \\
  \hline
  def setInitialMaxDepth(depth: Int) & Set the initial depth of \\
  & newly-grown trees. \\
  \hline
  \end{tabular}
  \caption{Scala API for the incremental random forest (RF) classifier developed in this thesis.}
  \label{tab:api}
\end{table}
